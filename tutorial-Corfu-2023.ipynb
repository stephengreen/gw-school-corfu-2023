{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf826081-c9ef-4aff-ba53-8e9b573ca2d8",
   "metadata": {},
   "source": [
    "# Tutorial on GW Parameter Inference with Machine Learning \n",
    "\n",
    "## Kavli-Villum Summer School on Gravitational Waves\n",
    "\n",
    "### Stephen Green *stephen.green2@nottingham.ac.uk*\n",
    "\n",
    "---\n",
    "\n",
    "In this tutorial we will build a simple neural network to do **parameter estimation** using simulation-based inference.\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. **Build a set of training waveforms.** We use the IMRPhenomPv2 model, parametrized only by masses.\n",
    "2. **Build a posterior model $q(\\theta | d)$** using a neural network. We consider two cases:\n",
    "    - Gaussian distribution with diagonal (learnable) covariance.\n",
    "    - Normalizing flow (masked autoregressive flow).\n",
    "3. **Train the model to represent the posterior**, i.e., $q(\\theta|d) \\to p(\\theta|d)$.\n",
    "    - During training, we add noise to waveforms to make simulated data.\n",
    "4. **Evaluate** on test data.\n",
    "\n",
    "This should run in a few minutes on a laptop.\n",
    "\n",
    "### Exercises\n",
    "\n",
    "1. Add new parameters, beyond the masses\n",
    "2. Use a more complicated waveform model\n",
    "3. Make a PP plot\n",
    "4. Experiment with the spline flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213733c5-f5c3-484b-a9b0-f15f4a8fb0c3",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "We use the [PyTorch](https://pytorch.org) ML library. Waveforms are generated using `lalsimulation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3272e6e5-d0be-45a6-b8ba-12c78b73a966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using colab, install appropriate packages.\n",
    "!pip install corner\n",
    "!pip install lalsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3ab79e-5a09-4f9b-86e8-29d3a09beee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import lal\n",
    "import lalsimulation as LS\n",
    "import corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa639f0f-2579-4243-aae2-52180187d97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d101ea-a62c-42f5-8cce-a0d40e4930c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running on Apple Silicon or CUDA is available, use the GPU.\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181558eb-0df5-49bb-b773-4d3de235f767",
   "metadata": {},
   "source": [
    "## Training data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b1b8b45f-b62b-478f-9cd2-cf8f7161a736",
   "metadata": {},
   "source": [
    "### Signal model\n",
    "\n",
    "Generic quasi-circular inspirals depend on 15 parameters $\\theta$:\n",
    "* Masses (2 parameters)\n",
    "* Spins (6 parameters)\n",
    "* Inclination\n",
    "* Sky position (2 parameters)\n",
    "* Luminosity distance\n",
    "* Time of arrival\n",
    "* Polarization\n",
    "* Phase\n",
    "\n",
    "We can write $h_I(\\theta)$ for the signal in detector $I$.\n",
    "\n",
    "However, for this tutorial we will restrict consideration to only the masses, fixing everything else. Rather than considering waveforms in detectors, we will also only analyze the plus polarization.\n",
    "\n",
    "**Exercise:** Perform inference for more parameters, e.g., luminosity distance, inclination, phase, or aligned spins. (For sky position, it is necessary to project the polarizations onto detectors.)\n",
    "\n",
    "### Noise\n",
    "\n",
    "Ultimately we must train on simulated data, which also include noise,\n",
    "$$\n",
    "d = h(\\theta) + n, \\qquad n \\sim p_{S_n}(n).\n",
    "$$\n",
    "The noise is taken to be stationary Gaussian with some power spectral density $S_n$.\n",
    "\n",
    "Rather than creating complete simulated data sets in advance of training, **we only prepare the waveforms in advance, and we add noise realizations during training.** The reason for this is that we would like the training dataset to be as large as possible to reduce the risk of overfitting. Noise is fast to sample, so this can be done during training, and doing so effectively makes the training set much larger. Generally waveforms are slower to generate, so we re-use them in each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0e39ea-f79b-4055-9462-eb18fd37e78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of the training set.\n",
    "num_samples = 10000\n",
    "\n",
    "# Sample the masses over a uniform prior.\n",
    "m_lower = 20.0  # solar masses\n",
    "m_upper = 80.0\n",
    "masses = m_lower + np.random.random((num_samples, 2)) * (m_upper - m_lower)\n",
    "\n",
    "# Make sure m1 > m2\n",
    "masses = np.sort(masses, axis=-1)\n",
    "masses = np.flip(masses, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658f2fda-270f-4895-8dc6-23079de2b42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed parameters\n",
    "\n",
    "distance = 1000  # Mpc\n",
    "inclination = 0.0  # face on\n",
    "spin1x = 0.0\n",
    "spin1y = 0.0\n",
    "spin1z = 0.0\n",
    "spin2x = 0.0\n",
    "spin2y = 0.0\n",
    "spin2z = 0.0\n",
    "phase = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb451fda-85f5-42d4-93db-0fc0d5fe3dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Waveform settings\n",
    "\n",
    "approximant = 'IMRPhenomPv2'\n",
    "f_min = 20.0\n",
    "f_max = 512.0\n",
    "T = 2.0\n",
    "\n",
    "delta_f = 1 / T\n",
    "nf = int(f_max / delta_f) + 1\n",
    "f_array = np.linspace(0.0, f_max, num=nf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1ae30d-44bf-42b0-91fa-14de51322481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the noise PSD. We take this to be the LIGO design sensitivity.\n",
    "\n",
    "lalseries = lal.CreateREAL8FrequencySeries('', lal.LIGOTimeGPS(0), 0, delta_f, lal.DimensionlessUnit, nf)\n",
    "LS.SimNoisePSD(lalseries, 0, LS.SimNoisePSDaLIGOZeroDetHighPowerPtr)\n",
    "psd = lalseries.data.data\n",
    "psd[:int(f_min/delta_f)] = 1\n",
    "psd[-1] = psd[-2]\n",
    "\n",
    "asd = np.sqrt(psd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffbbab9-997f-4b1d-98d3-8d2da5ed720e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ASD.\n",
    "\n",
    "plt.plot(f_array, asd)\n",
    "plt.xlim((f_min, f_max))\n",
    "plt.ylim((3e-24, 3e-23))\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('f [Hz]')\n",
    "plt.ylabel('ASD')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2aeb73-a658-4804-b528-a0e3abf302bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training waveforms\n",
    "\n",
    "hp_list = []\n",
    "hc_list = []\n",
    "\n",
    "for i in range(num_samples):\n",
    "    \n",
    "    mass1, mass2 = masses[i]\n",
    "\n",
    "    mass1_lal = mass1 * lal.MSUN_SI\n",
    "    mass2_lal = mass2 * lal.MSUN_SI\n",
    "    distance_lal = distance * 1e6 * lal.PC_SI\n",
    "    approximant_lal = LS.GetApproximantFromString(approximant)\n",
    "\n",
    "    hp, hc = LS.SimInspiralFD(\n",
    "        mass1_lal, mass2_lal,\n",
    "        spin1x, spin1y, spin1z, spin2x, spin2y, spin2z, \n",
    "        distance_lal, inclination, phase,\n",
    "        0.0, 0.0, 0.0,  # longAscNodes, eccentricity, meanPerAno\n",
    "        delta_f, f_min, f_max, 20.0,  # f_ref\n",
    "        None,\n",
    "        approximant_lal,\n",
    "    )\n",
    "\n",
    "    dt = 1 / hp.deltaF + (hp.epoch.gpsSeconds + hp.epoch.gpsNanoSeconds * 1e-9)\n",
    "    time_shift = np.exp(-1j * 2 * np.pi * dt * f_array)\n",
    "    \n",
    "    hp = hp.data.data\n",
    "    hc = hc.data.data\n",
    "\n",
    "    hp *= time_shift\n",
    "    hc *= time_shift\n",
    "    \n",
    "    hp[:int(f_min/delta_f)] = 0.0\n",
    "    hc[:int(f_min/delta_f)] = 0.0\n",
    "    \n",
    "    # Whiten waveforms and rescale so that white noise has unit variance in each frequency bin.\n",
    "    # This makes it easy to add noise, and also normalized data works better as input to neural networks.\n",
    "    \n",
    "    hp = hp / asd * np.sqrt(4.0 * delta_f)\n",
    "    hc = hc / asd * np.sqrt(4.0 * delta_f)\n",
    "    \n",
    "    hp_list.append(hp)\n",
    "    hc_list.append(hc)\n",
    "\n",
    "hp = np.array(hp_list)\n",
    "hc = np.array(hc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b603a8-ce6e-4202-a2e5-c85175fb308c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample waveform\n",
    "\n",
    "plt.plot(f_array, hp[0].real)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('$f$')\n",
    "plt.ylabel('Re $h_+$')\n",
    "plt.xlim((10, f_max))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6a557d-38c1-41a6-a038-419f0d13425d",
   "metadata": {},
   "source": [
    "### Package into a PyTorch Dataset\n",
    "\n",
    "The `Dataset` is a convenient class for storing and accessing pairs of parameters and associated data. It must define the following methods:\n",
    "\n",
    "* `__len__()`: Return total number of samples in the dataset.\n",
    "* `__getitem__(idx)`: Retrieve a $(\\theta, d)$ pair of parameters and data. We use this method to also add (in real time) a noise realization to each simulated waveform. (Therefore repeated calls will give different noise realizations).\n",
    "\n",
    "For now, we will only make use of the plus polarization for simplicity.\n",
    "\n",
    "**Exercise:** Extend this to include more parameters. Some parameters, e.g., luminosity distance, can be easily transformed. For these, it makes sense to select them in real time, like for the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921f457f-3256-43c8-9e99-9ee958462c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "#\n",
    "# This is just the masses. It's better to sample in (Mc, q) rather than (m1, m2) because the posterior is more Gaussian.\n",
    "\n",
    "m1 = masses[:, 0]\n",
    "m2 = masses[:, 1]\n",
    "\n",
    "Mc = (m1 * m2)**(3/5) / (m1 + m2)**(1/5)\n",
    "q = m2 / m1\n",
    "\n",
    "parameters = np.stack((Mc, q), axis=1).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583a1d63-01d0-42db-bbf2-7db0adc598a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For best training, parameters should be standardized (zero mean, unit variance across the training set)\n",
    "\n",
    "parameters_mean = np.mean(parameters, axis=0)\n",
    "parameters_std = np.std(parameters, axis=0)\n",
    "\n",
    "parameters_standardized = (parameters - parameters_mean) / parameters_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b6bd1a-040f-4bf1-99c9-284ce01fa049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Waveforms\n",
    "#\n",
    "# Truncate the arrays to remove zeros below f_min, and repackage real and imaginary parts\n",
    "#\n",
    "# Only consider h_plus for now\n",
    "\n",
    "lower_cut = int(f_min / delta_f)\n",
    "waveforms = np.hstack((hp.real[:, lower_cut:], hp.imag[:, lower_cut:])).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee096ac-93b8-4725-bec5-2268ab416d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveformDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, parameters, waveforms):\n",
    "        self.parameters = parameters\n",
    "        self.waveforms = waveforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.parameters)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        params = self.parameters[idx]\n",
    "        signal = self.waveforms[idx]\n",
    "        \n",
    "        # Add unit normal noise to the signal\n",
    "        noise = np.random.normal(size = signal.shape).astype(np.float32)\n",
    "        data = signal + noise\n",
    "        \n",
    "        return torch.tensor(data, device=device), torch.tensor(params, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9962f84b-f12b-42e8-ad63-d713fd026c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform_dataset = WaveformDataset(parameters_standardized, waveforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b3df66-380e-475e-805e-bd8c0adb32dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can sample from the WaveformDataset. This gives us pairs of data and parameters, different noise realizations each time.\n",
    "\n",
    "x, y = waveform_dataset[0]\n",
    "x = x.cpu()  # Careful to put data back onto CPU.\n",
    "plt.plot(x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b446329-e941-4fca-b782-f0721f6342f2",
   "metadata": {},
   "source": [
    "## Gaussian posterior model\n",
    "\n",
    "We now try to fit the posterior using a Gaussian model, i.e.,\n",
    "\n",
    "$$\n",
    "q(\\theta|d) = \\mathcal{N}(\\mu(d), \\Sigma(d)) (\\theta).\n",
    "$$\n",
    "Here $\\mu$ and $\\Sigma$ are the mean and covariance matrix describing the Gaussian, and they are given as the output of a neural network, which takes as input the data $d$. Additionally, we constrain $\\Sigma$ to be diagonal, so we only learn the variance in each parameter, and we do not allow for correlations between parameters in the posterior. (**Exercise:** Lift this restriction.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5a15e1-7686-4f4d-a02c-23b6d035833a",
   "metadata": {},
   "source": [
    "### Model construction\n",
    "\n",
    "Neural networks are constructed by subclassing `nn.Module`.\n",
    "\n",
    "This has to implement an `__init__()` and `forward()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30d2fb4-969c-4c83-ba1d-ffcb0024e6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural networks are constructed by subclassing nn.Module\n",
    "#\n",
    "# This has to implement an __init__() and forward() method\n",
    "\n",
    "class GaussianNeuralNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dims, output_dim, activation=nn.ReLU()):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        # Hidden layers\n",
    "        hidden_net_list = []\n",
    "        hidden_net_list.append(\n",
    "            nn.Linear(input_dim, hidden_dims[0]))\n",
    "        for i in range(1, len(hidden_dims)):\n",
    "            hidden_net_list.append(nn.Linear(hidden_dims[i-1], hidden_dims[i]))\n",
    "        self.hidden_net_list = nn.ModuleList(hidden_net_list)\n",
    "        \n",
    "        # Output layers\n",
    "        self.output_mean = nn.Linear(hidden_dims[-1], output_dim)\n",
    "        self.output_log_sigma = nn.Linear(hidden_dims[-1], output_dim)\n",
    "        \n",
    "        # Activation function\n",
    "        self.activation = activation\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Pass x through all the layers of the network and return the Gaussian distribution\"\"\"\n",
    "        \n",
    "        h = x\n",
    "        for layer in self.hidden_net_list:\n",
    "            h = self.activation(layer(h))\n",
    "\n",
    "        # Output layer defines a Gaussian\n",
    "        mean = self.output_mean(h)\n",
    "        log_sigma = self.output_log_sigma(h)\n",
    "        sigma = torch.exp(log_sigma)\n",
    "        \n",
    "        # Create the Gaussian distribution\n",
    "        dist = torch.distributions.MultivariateNormal(loc=mean, scale_tril=torch.diag_embed(sigma))\n",
    "        \n",
    "        return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7cdfa4-1fe8-4993-a5f0-311889bba38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = waveforms.shape[-1]\n",
    "output_dim = parameters.shape[-1]\n",
    "hidden_dims = [512, 256, 128, 64, 32]\n",
    "\n",
    "model = GaussianNeuralNetwork(input_dim, hidden_dims, output_dim)\n",
    "\n",
    "model.to(device)  # Put model on GPU, if available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee627d9-db98-4d9f-82f6-99e51264de02",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232f5f26-796f-484c-b52c-a035090ecb02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the dataset into training and test sets.\n",
    "# We use the test set to make sure the network properly generalizes to data that it has not seen in training, i.e., it does not overfit.\n",
    "\n",
    "train_fraction = 0.8\n",
    "num_train = int(round(train_fraction * num_samples))\n",
    "num_test = num_samples - num_train\n",
    "train_dataset, test_dataset = random_split(waveform_dataset, [num_train, num_test])\n",
    "\n",
    "# The DataLoader is used to obtain samples form the Dataset during training.\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16304af3-c970-416a-8520-1db669330f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The DataLoaders iterate over samples, returning torch tensors containing a batch of data\n",
    "\n",
    "train_features, train_labels = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502834d2-fb64-484a-8fa6-3616e3633a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5541e3-1a5a-435c-b0f2-98b8bddcfb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff8e2e9-f485-485d-8bc2-fc31d74afe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the Adam optimizer.\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594b543d-e129-4845-ace5-f6a387d6ad41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and test loops\n",
    "\n",
    "def train_loop(dataloader, model, optimizer):\n",
    " \n",
    "    size = len(dataloader.dataset)\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute negative log probability loss\n",
    "        dist = model(X)        \n",
    "        loss = - dist.log_prob(y)\n",
    "        \n",
    "        train_loss += loss.detach().sum()\n",
    "        loss = loss.mean()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 50 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"Loss: {loss:>7f}  [{current:>5d}/{size:>5d} samples]\")\n",
    "            \n",
    "    average_loss = train_loss.item() / size\n",
    "    print('Average loss: {:.4f}'.format(average_loss))\n",
    "    return average_loss\n",
    "            \n",
    "        \n",
    "        \n",
    "def test_loop(dataloader, model):\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            dist = model(X)\n",
    "            loss = - dist.log_prob(y)\n",
    "            test_loss += loss.sum()\n",
    "\n",
    "    test_loss = test_loss.item() / size\n",
    "    print(f\"Test loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a8d1ff-461a-4e1b-a11f-82946561b876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for 20 epochs. \n",
    "\n",
    "epochs = 20\n",
    "train_history = []\n",
    "test_history = []\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    loss = train_loop(train_dataloader, model, optimizer)\n",
    "    train_history.append(loss)\n",
    "    loss = test_loop(test_dataloader, model)\n",
    "    test_history.append(loss)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e2c6a2-9541-408a-9ff8-b2273da49e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.arange(1, len(train_history) + 1)\n",
    "plt.plot(epochs, train_history, label = 'train loss')\n",
    "plt.plot(epochs, test_history, label = 'test loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffe0fb3-56c7-4f5a-9db1-9fc0655311f1",
   "metadata": {},
   "source": [
    "We see that the test loss drops just as fast as the train. Since these curves coincide we conclude that the model is not overfitting.\n",
    "\n",
    "If we trained longer, or used a more powerful model, we might find a difference between these curves. If this happens, increase the size of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509c6a64-cee3-4ff8-9683-59fd24abe223",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_posteriors = 5\n",
    "num_samples = 10000\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for n in range(num_posteriors):\n",
    "    with torch.no_grad():\n",
    "        test_x, test_y = test_dataset[n]\n",
    "    \n",
    "        # Predict a posterior\n",
    "        dist = model(test_x)\n",
    "    \n",
    "        # Sample the posterior\n",
    "        pred_samples = dist.sample((10000,)).cpu().numpy()\n",
    "\n",
    "        # Undo the standardization\n",
    "        pred_samples = parameters_std * pred_samples + parameters_mean\n",
    "        truth = parameters_std * test_y.cpu().numpy() + parameters_mean\n",
    "\n",
    "    # Plot\n",
    "    corner.corner(pred_samples, truths=truth, labels=['$M_c$', '$q$'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e847b47-eda5-4ac3-a6eb-494d3d508ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_posteriors = 5\n",
    "num_eval_samples = 10000\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for n in range(num_posteriors):\n",
    "\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        test_x, test_y = test_dataset[n]\n",
    "       \n",
    "        # Sample the posterior\n",
    "        test_x = test_x.expand(num_eval_samples, *x.shape)\n",
    "        pred_samples = model.sample(1, test_x).squeeze(1).cpu().numpy()\n",
    "    \n",
    "        # Undo the standardization\n",
    "        pred_samples = parameters_std * pred_samples + parameters_mean\n",
    "        truth = parameters_std * test_y.cpu().numpy() + parameters_mean\n",
    "    \n",
    "        # Plot\n",
    "        corner.corner(pred_samples, truths=truth, labels=['$M_c$', '$q$'])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece3029d-3f93-4f0c-b0eb-48901d9ef011",
   "metadata": {},
   "source": [
    "## Normalizing flow\n",
    "\n",
    "Normalizing flows allow for much more general probability distributions, as needed to represent GW posterior distributions in 15D. A normalizing flow represents the distribution in terms of a mapping $f_d:\\mathbb R^D \\to \\mathbb R^D$ from a *base distribution* $\\pi(u) = \\mathcal N(0,1)^D$,\n",
    "$$\n",
    "q(\\theta | d) = \\pi(f_d^{-1}(\\theta)) | \\det J_{f_d}^{-1} |.\n",
    "$$\n",
    "To sample $\\theta \\sim q(\\theta|d)$, first sample $u\\sim \\pi(u)$, then apply the mapping and set $\\theta = f_d(u)$. By having $f_d$ depend on the data $d$, this describes a conditional distribution, as needed for amortized inference.\n",
    "\n",
    "To train the flow, it is necessary also to evaluate the density. This can be done by evaluating the right hand side of the equation above, provided two properties hold:\n",
    "1. $f_d$ invertible\n",
    "2. $f_d$ has simple Jacobian determinant\n",
    "   \n",
    "The trick then is to construct suitable $f_d$ such that these properties hold. A simple model is a [Masked Autoregressive Flow (MAF)](https://arxiv.org/abs/1705.07057)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11f008b-f7db-4bb4-b7a3-6ff763d8f826",
   "metadata": {},
   "source": [
    "### Model construction\n",
    "\n",
    "Normalizing flows are constructed as a sequence of discrete steps, each of which is relatively simple. For an autoregressive flow step $n$, we write\n",
    "$$\n",
    "q^{(n)}(\\theta | d) = \\prod_{i=1}^D q^{(n)}(\\theta_i | \\theta_{1:i-1}, d),\n",
    "$$\n",
    "where each factor takes a particular parametrized form. For the MAF, this is taken to be a univariate normal distribution,\n",
    "$$\n",
    "q^{(n)}(\\theta_i | \\theta_{1:i-1}, d) = \\mathcal{N}\\left(\\mu_i^{(n)}(\\theta_{1:i-1},d), \\sigma_i^{(n)}(\\theta_{1:i-1},d)\\right)\n",
    "$$\n",
    "so that the corresponding mapping is an affine transformation. The means and variances are given as output of a neural network, and are learned. There is in fact a smart way of representing all of these quantities for a single step in one neural network with masking called [MADE](https://arxiv.org/abs/1502.03509). Between each flow step we also randomize the order of the parameters.\n",
    "\n",
    "For the normalizing flow, we use an off-the-shelf library rather than coding it ourselves. Although we will use a MAF for our example, code for a [spline flow](https://arxiv.org/abs/1906.04032) is also included. **Exercise:** Try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128c4ed0-0149-4e17-b9bf-1f6fbe3141f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The glasflow library is a wrapper for nflows (https://github.com/bayesiains/nflows), which implements many different flows.\n",
    "\n",
    "from glasflow.nflows import transforms, distributions, flows\n",
    "import glasflow.nflows as nflows\n",
    "import glasflow.nflows.nn.nets as nflows_nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e062aa-1604-4de5-91bd-f7453410b98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'  # For some reason, \"mps\" does not work here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced95323-20a3-423a-a0fe-f05502898f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here is adapted from examples in the neural spline flow repository, https://github.com/bayesiains/nsf\n",
    "\n",
    "# These functions create the transform sequence.\n",
    "\n",
    "def create_base_transform(\n",
    "    i: int,\n",
    "    param_dim: int,\n",
    "    context_dim: int = None,\n",
    "    hidden_dim: int = 512,\n",
    "    num_transform_blocks: int = 2,\n",
    "    batch_norm: bool = False,\n",
    "    base_transform_type: str = \"maf\",\n",
    "):\n",
    "    \n",
    "    activation_fn = nn.ELU()\n",
    "\n",
    "    if base_transform_type == \"maf\":\n",
    "        return transforms.MaskedAffineAutoregressiveTransform(\n",
    "            param_dim,\n",
    "            hidden_features=hidden_dim,\n",
    "            context_features=context_dim,\n",
    "            num_blocks=num_transform_blocks,\n",
    "            activation=activation_fn,\n",
    "            use_batch_norm=batch_norm,\n",
    "        )\n",
    "\n",
    "    elif base_transform_type == \"rq-coupling\":\n",
    "        if param_dim == 1:\n",
    "            mask = torch.tensor([1], dtype=torch.uint8)\n",
    "        else:\n",
    "            mask = nflows.utils.create_alternating_binary_mask(\n",
    "                param_dim, even=(i % 2 == 0)\n",
    "            )\n",
    "        return transforms.PiecewiseRationalQuadraticCouplingTransform(\n",
    "            mask=mask,\n",
    "            transform_net_create_fn=(\n",
    "                lambda in_features, out_features: nflows_nets.ResidualNet(\n",
    "                    in_features=in_features,\n",
    "                    out_features=out_features,\n",
    "                    hidden_features=hidden_dim,\n",
    "                    context_features=context_dim,\n",
    "                    num_blocks=num_transform_blocks,\n",
    "                    activation=activation_fn,\n",
    "                    use_batch_norm=batch_norm,\n",
    "                )\n",
    "            ),\n",
    "            num_bins=8,\n",
    "            tails=\"linear\",\n",
    "            tail_bound=1.0,\n",
    "            apply_unconditional_transform=False,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "\n",
    "def create_linear_transform(param_dim: int):\n",
    "    return transforms.CompositeTransform(\n",
    "        [\n",
    "            transforms.RandomPermutation(features=param_dim),\n",
    "            transforms.LULinear(param_dim, identity_init=True),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def create_transform(\n",
    "    num_flow_steps: int, param_dim: int, context_dim: int, base_transform_kwargs: dict\n",
    "):\n",
    "    return transforms.CompositeTransform(\n",
    "        [\n",
    "            transforms.CompositeTransform(\n",
    "                [\n",
    "                    create_linear_transform(param_dim),\n",
    "                    create_base_transform(\n",
    "                        i, param_dim, context_dim=context_dim, **base_transform_kwargs\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "            for i in range(num_flow_steps)\n",
    "        ]\n",
    "        + [create_linear_transform(param_dim)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc22810a-f180-4171-9f5e-7d43c591a13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The entire flow model consists of a standard normal base distribution, followed by the transform.\n",
    "\n",
    "transform = create_transform(num_flow_steps = 3,\n",
    "                             param_dim = parameters.shape[-1],\n",
    "                             context_dim = waveforms.shape[-1],\n",
    "                             base_transform_kwargs = {\"base_transform_type\": \"maf\"})\n",
    "\n",
    "base_distribution = distributions.StandardNormal(shape=[parameters.shape[-1]])\n",
    "\n",
    "model = flows.Flow(transform, base_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071cbb08-e834-4931-8b31-6ee2e62b3fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f085a789-042c-48b2-b214-fa3d3c056f05",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368f3480-b6fd-4826-a132-25f17fb6ab54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and test sets\n",
    "\n",
    "train_fraction = 0.8\n",
    "num_train = int(round(train_fraction * num_samples))\n",
    "num_test = num_samples - num_train\n",
    "train_dataset, test_dataset = random_split(waveform_dataset, [num_train, num_test])\n",
    "\n",
    "# The DataLoader is used in training\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3a561b-cc2d-4649-b5ef-42ec2091c2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the Adam optimizer.\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe428aff-ac60-407f-8f72-cb25645cd545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and test loops\n",
    "\n",
    "def train_loop(dataloader, model, optimizer):\n",
    "\n",
    "    model.train()\n",
    " \n",
    "    size = len(dataloader.dataset)\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute negative log probability loss  \n",
    "        loss = - model.log_prob(y, X)\n",
    "        \n",
    "        train_loss += loss.detach().sum()\n",
    "        loss = loss.mean()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 50 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"Loss: {loss:>7f}  [{current:>5d}/{size:>5d} samples]\")\n",
    "            \n",
    "    average_loss = train_loss.item() / size\n",
    "    print('Average loss: {:.4f}'.format(average_loss))\n",
    "    return average_loss\n",
    "            \n",
    "        \n",
    "        \n",
    "def test_loop(dataloader, model):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            loss = - model.log_prob(y, X)\n",
    "            test_loss += loss.sum()\n",
    "\n",
    "    test_loss = test_loss.item() / size\n",
    "    print(f\"Test loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43eabe3b-443b-4bdb-b07a-0c531c3a7402",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "train_history = []\n",
    "test_history = []\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    loss = train_loop(train_dataloader, model, optimizer)\n",
    "    train_history.append(loss)\n",
    "    loss = test_loop(test_dataloader, model)\n",
    "    test_history.append(loss)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c63a91-0eda-472c-876f-bf7eedd1705e",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.arange(1, len(train_history) + 1)\n",
    "plt.plot(epochs, train_history, label = 'train loss')\n",
    "plt.plot(epochs, test_history, label = 'test loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f532f965-856d-4bc9-99a0-6e8948d383c5",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f695a5-95bb-433d-9653-3d58dc8f6606",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_posteriors = 5\n",
    "num_eval_samples = 10000\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for n in range(num_posteriors):\n",
    "\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        test_x, test_y = test_dataset[n]\n",
    "       \n",
    "        # Sample the posterior\n",
    "        test_x = test_x.expand(num_eval_samples, *x.shape)\n",
    "        pred_samples = model.sample(1, test_x).squeeze(1).cpu().numpy()\n",
    "    \n",
    "        # Undo the standardization\n",
    "        pred_samples = parameters_std * pred_samples + parameters_mean\n",
    "        truth = parameters_std * test_y.cpu().numpy() + parameters_mean\n",
    "    \n",
    "        # Plot\n",
    "        corner.corner(pred_samples, truths=truth, labels=['$M_c$', '$q$'])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaa3c2e-aa61-48c1-b16f-e6ec52abd00e",
   "metadata": {},
   "source": [
    "**Exercise:** Try learning $(m_1, m_2)$ rather than $(M_c, q)$. The flow should be able to represent the distinct shape."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
